{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# from ipdb import set_trace as debug\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Methods!\n",
    "def new_state(new_input, old_state):\n",
    "    # in the description of the hw there were no bias terms so i did not include them either\n",
    "    return np.tanh(np.dot(Wxh, new_input) + np.dot(Whh, old_state))\n",
    "\n",
    "def one_hot_encoded(index):\n",
    "    temp = np.zeros((vocab_size,1))\n",
    "    temp[index] = 1\n",
    "    return temp\n",
    "\n",
    "def softmax_output(state):\n",
    "    logit = np.dot(Why, state) # the logits\n",
    "    # apply softmax\n",
    "    return np.exp(logit) / np.sum(np.exp(logit)) # probabilities for next chars\n",
    "\n",
    "def cross_entropy_loss(probabilities, y_trues):\n",
    "    loss = 0\n",
    "    for t in range(len(y_trues)):\n",
    "        index_of_true_char = y_trues[t]\n",
    "        loss+= -np.log(probabilities[t][index_of_true_char,0])\n",
    "    return loss\n",
    "\n",
    "def forward_pass(inputs, hprev):\n",
    "    xs, hs, ps = {}, {}, {}         # using dict instead of arrays makes more readable code\n",
    "    \n",
    "    # Note: a trick that makes the loop below smooth\n",
    "    # unless we do this we will need to handle  hs[t-1] case at t=0\n",
    "    hs[-1] = np.copy(hprev) \n",
    "                                       \n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        xs[t] = one_hot_encoded(index=inputs[t])\n",
    "        \n",
    "        hs[t] = new_state(xs[t], hs[t-1])\n",
    "\n",
    "        ps[t] = softmax_output(state=hs[t])\n",
    "    \n",
    "    last_state =  hs[len(inputs)-1] # will be new previous state\n",
    "    # why 'len(inputs)-1' instead of just -1 you ask? Because this is a dict and not a list \n",
    "    # and -1 is used for the previous state. see note above\n",
    "\n",
    "    return xs, hs, ps, last_state\n",
    "\n",
    "def copy_zeros_in_shape_of_these(*these):\n",
    "    return tuple(np.zeros_like(one) for one in these)\n",
    "\n",
    "def backward_pass(xs, hs, ps, y_trues):\n",
    "    dWxh, dWhh, dWhy = copy_zeros_in_shape_of_these(Wxh, Whh, Why)\n",
    "    dh_from_next_state = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[y_trues[t]] -= 1 # gradient from softmax\n",
    "                \n",
    "        dWhy += np.dot(dy, hs[t].T) # y_t = Why * h_t ==> dWhy =  dy_t * transpose(h_t)\n",
    "\n",
    "        \n",
    "        dh_from_dy = np.dot(Why.T, dy) # derivative of dh from the output i.e.: y_t = Why * h_t\n",
    "        dh = dh_from_dy + dh_from_next_state # h affects current y and also 'next state'        \n",
    "        \n",
    "        one_minus_tanhSquared = (1 - hs[t] * hs[t]) # note that hs[t] IS tanh\n",
    "        # see https://socratic.org/questions/what-is-the-derivative-of-tanh-x\n",
    "        \n",
    "        # from high school: içinin türevi çarpı kendinin türevi (f = tanh iken)\n",
    "        dh_over_dtanh =  dh * one_minus_tanhSquared  # handle tanh fucntion\n",
    "        \n",
    "        dWxh += np.dot(dh_over_dtanh, xs[t].T)\n",
    "        dWhh += np.dot(dh_over_dtanh, hs[t-1].T)\n",
    "        \n",
    "        dh_from_next_state = np.dot(Whh.T, dh_over_dtanh) # this will be used in the next step \n",
    "        # which is the previous state because we are going backwards\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy]:\n",
    "        # if some gradients get too large causes exploiding gradients,\n",
    "        # one heurustic is limiting the value:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # inplace clipping\n",
    "        # see https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html\n",
    "        \n",
    "    return dWxh, dWhh, dWhy\n",
    "    \n",
    "    \n",
    "def update_parameters():\n",
    "    if optimization == 'adagrad':\n",
    "        for param, dparam, adagrad_mem in zip([Wxh, Whh, Why], \n",
    "                                     [dWxh, dWhh, dWhy], \n",
    "                                     [mWxh, mWhh, mWhy]):\n",
    "\n",
    "                adagrad_mem += np.square(dparam)\n",
    "                 # numpy elementwise multiplies \\ divides \\ sqrt's\n",
    "                param += -learning_rate * (dparam / np.sqrt(adagrad_mem + 1e-8))\n",
    "    elif optimization == 'sgd':\n",
    "        print('SGD sucks man!')\n",
    "        for param, dparam in zip([Wxh, Whh, Why], \n",
    "                                     [dWxh, dWhh, dWhy]):\n",
    "            param += -learning_rate/10 * dparam\n",
    "\n",
    "            \n",
    "def sample(initial_state, seed_char_index, n):\n",
    "    indexs = [seed_char_index]\n",
    "    for _ in range(n):\n",
    "        xs, hs, ps, initial_state = forward_pass([seed_char_index],initial_state)\n",
    "        seed_char_index = np.random.choice(range(vocab_size), p=ps[0].ravel())\n",
    "        indexs.append(seed_char_index)\n",
    "    return indexs\n",
    "\n",
    "\n",
    "def print_sample(initial_state, seed_char_index, n):\n",
    "    print(''.join(ix_to_char[ix] for ix in sample(hprev, inputs[0], n)))\n",
    "\n",
    "\n",
    "def sliding_window(text, width):\n",
    "    length = len(text)\n",
    "    for i in range(length-width):\n",
    "        yield text[i:i+width], text[i+1:i+width+1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are some trials on paper reviews data set 'https://www.kaggle.com/ahmaurya/iclr2017reviews/' I use only abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I take only one paper abstract and run all epochs on it, when there are few possible chars RNN overfits and actually puts some real words when sampled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 979 characters, 40 are unique\n",
      " - 1. epoch\n",
      "1. abstract\n",
      "average epoch loss 28.263477026597336\n",
      " - 2. epoch\n",
      "1. abstract\n",
      "average epoch loss 22.18818062705942\n",
      " - 3. epoch\n",
      "1. abstract\n",
      "average epoch loss 20.258261837519267\n",
      " - 4. epoch\n",
      "1. abstract\n",
      "average epoch loss 19.440408649204738\n",
      " - 5. epoch\n",
      "1. abstract\n",
      "average epoch loss 19.41274780627505\n",
      " - 6. epoch\n",
      "1. abstract\n",
      "average epoch loss 18.115550484174843\n",
      " - 7. epoch\n",
      "1. abstract\n",
      "average epoch loss 17.761373508970326\n",
      " - 8. epoch\n",
      "1. abstract\n",
      "average epoch loss 17.857820465854278\n",
      " - 9. epoch\n",
      "1. abstract\n",
      "average epoch loss 17.73807071348975\n",
      " - 10. epoch\n",
      "1. abstract\n",
      "average epoch loss 17.25307296236233\n",
      " - 11. epoch\n",
      "1. abstract\n",
      "average epoch loss 17.403057304628394\n",
      " - 12. epoch\n",
      "1. abstract\n",
      "average epoch loss 17.065101122294386\n",
      " - 13. epoch\n",
      "1. abstract\n",
      "average epoch loss 17.019714867783623\n",
      " - 14. epoch\n",
      "1. abstract\n",
      "average epoch loss 16.20878218283214\n",
      " - 15. epoch\n",
      "1. abstract\n",
      "average epoch loss 15.75983146278173\n",
      " - 16. epoch\n",
      "1. abstract\n",
      "average epoch loss 15.94037863517492\n",
      " - 17. epoch\n",
      "1. abstract\n",
      "average epoch loss 16.002803136900123\n",
      " - 18. epoch\n",
      "1. abstract\n",
      "average epoch loss 15.629090659449586\n",
      " - 19. epoch\n",
      "1. abstract\n",
      "average epoch loss 15.782924445460102\n",
      " - 20. epoch\n",
      "1. abstract\n",
      "average epoch loss 15.534178307579195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_data(n):\n",
    "    return list(pd.read_csv('iclr2017_papers.csv')['abstract'].dropna())[:n]\n",
    "\n",
    "# init everything!\n",
    "abstracts = get_data(1)\n",
    "data = '\\n'.join(abstracts)\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print('There are {} characters, {} are unique'.format(len(data), vocab_size))\n",
    "\n",
    "# neet idea for indexing chars back and forth,  better than dedicated function in my opinion\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "hidden_units = vocab_size*4//5 # size of hidden layer of neurons\n",
    "seq_length = 18 # recursion number for RNN so called unrolling\n",
    "learning_rate = 1e-1 # make this 1e-2 for sgd, even then it fluctuates very badly. maybe 1e-3 would be better\n",
    "optimization = 'adagrad'\n",
    "\n",
    "# trainable parameters\n",
    "Wxh = np.random.normal(scale=0.01,size=(hidden_units, vocab_size)) # input to hidden\n",
    "Whh = np.random.normal(scale=0.01,size=(hidden_units, hidden_units)) # hidden to hidden\n",
    "Why = np.random.normal(scale=0.01,size=(vocab_size, hidden_units)) # hidden to output\n",
    "\n",
    "        \n",
    "for epoch in (range(20)):\n",
    "    epoch_acc_loss = 0\n",
    "    print(' - ', epoch+1,'. epoch', sep='')\n",
    "    for num, abstract in enumerate(abstracts):\n",
    "        abstract_acc_loss = 0\n",
    "        print(num+1,'. abstract', sep='')\n",
    "        hprev = np.zeros((hidden_units,1)) # reset initila state = delete RNN's memory\n",
    "        if optimization == 'adagrad':\n",
    "            # the 'memory is needed for adagrad , standard sgd does not use them'\n",
    "            mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        \n",
    "        # loop in an abstract\n",
    "        for i, (inputs, expected_outputs) in enumerate(sliding_window(abstract, seq_length)):\n",
    "            inputs = list(map(lambda ch:char_to_ix[ch], inputs))\n",
    "            expected_outputs = list(map(lambda ch:char_to_ix[ch], expected_outputs))\n",
    "\n",
    "            xs, hs, ps, hprev = forward_pass(inputs, hprev)\n",
    "\n",
    "            loss = cross_entropy_loss(ps, expected_outputs)\n",
    "            abstract_acc_loss+=loss\n",
    "\n",
    "            dWxh, dWhh, dWhy = backward_pass(xs, hs, ps, expected_outputs)\n",
    "\n",
    "            update_parameters() # parameters are global\n",
    "            \n",
    "        abstract_avg_loss = abstract_acc_loss/(i+1)\n",
    "#         debug()\n",
    "        epoch_acc_loss+= abstract_avg_loss\n",
    "        \n",
    "    print('average epoch loss', epoch_acc_loss/(num+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eneconceperi astabilicpongueProort. Our tabirari, st rantecessary lik.: recursion. f to incessural nugges,or neural architecurante leh urarty necamonust ratecursioretabl nuicssto apor thate a corporate\n"
     ]
    }
   ],
   "source": [
    "print_sample(hprev, 20, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following takes first chars of first 10 abstarcts and runs epcohs on their combined text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11075 characters, 53 are unique\n",
      "1. epoch average loss 30.442283662474296\n",
      "2. epoch average loss 29.804240122335035\n",
      "3. epoch average loss 29.024874574189454\n",
      "4. epoch average loss 28.96572882440713\n",
      "5. epoch average loss 28.735255303056714\n",
      "6. epoch average loss 28.73013578900492\n",
      "7. epoch average loss 28.533745385714447\n",
      "8. epoch average loss 28.3132707742684\n",
      "9. epoch average loss 28.53253314656235\n",
      "10. epoch average loss 28.278888403310134\n",
      "11. epoch average loss 28.092286291463193\n",
      "12. epoch average loss 28.216757915585028\n",
      "13. epoch average loss 28.008012578099795\n",
      "14. epoch average loss 27.890203206792744\n",
      "15. epoch average loss 27.760775869816158\n",
      "16. epoch average loss 27.472150389136388\n",
      "17. epoch average loss 27.54186228855362\n",
      "18. epoch average loss 27.57271463106844\n",
      "19. epoch average loss 27.54594865754184\n",
      "20. epoch average loss 27.74426489801027\n",
      "21. epoch average loss 28.147911229596623\n",
      "22. epoch average loss 27.73776393230429\n",
      "23. epoch average loss 27.781105105937897\n",
      "24. epoch average loss 28.097120249018413\n",
      "25. epoch average loss 28.188914847814573\n",
      "26. epoch average loss 27.903558529268913\n",
      "27. epoch average loss 27.981766767013863\n",
      "28. epoch average loss 27.932122525232536\n",
      "29. epoch average loss 27.855774625569158\n",
      "30. epoch average loss 27.87957823149335\n",
      "31. epoch average loss 27.91463027186124\n",
      "32. epoch average loss 27.496404657538026\n",
      "33. epoch average loss 27.333945443611224\n",
      "34. epoch average loss 27.512686886351695\n",
      "35. epoch average loss 27.45782584836299\n",
      "36. epoch average loss 27.310155717354267\n",
      "37. epoch average loss 27.579327040851297\n",
      "38. epoch average loss 27.5649627956138\n",
      "39. epoch average loss 27.359189176059594\n",
      "40. epoch average loss 27.445126585500162\n",
      "41. epoch average loss 27.464957475360663\n",
      "42. epoch average loss 27.54474524504929\n",
      "43. epoch average loss 27.62179640485858\n",
      "44. epoch average loss 27.60711392582579\n",
      "45. epoch average loss 27.50420531930977\n",
      "46. epoch average loss 27.643884909990142\n",
      "47. epoch average loss 27.92927378985512\n",
      "48. epoch average loss 27.532276790690453\n",
      "49. epoch average loss 27.474934669162497\n",
      "50. epoch average loss 27.704134057080335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# init every thing!\n",
    "data = '\\n'.join(list(pd.read_csv('iclr2017_papers.csv')['abstract'].dropna())[:10]).lower()\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print('There are {} characters, {} are unique'.format(len(data), vocab_size))\n",
    "\n",
    "# neet way for indexing chars back and forth,  better than dedicated functions in my opinion\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "hidden_units = vocab_size*4//5 # size of hidden layer of neurons\n",
    "seq_length = 18 # recursion number for RNN so called unrolling\n",
    "learning_rate = 1e-1 # make this 1e-2 for sgd, even then it fluctuates very badly. maybe 1e-3 would be better\n",
    "optimization = 'adagrad'\n",
    "\n",
    "# trainable parameters\n",
    "Wxh = np.random.normal(scale=0.01,size=(hidden_units, vocab_size)) # input to hidden\n",
    "Whh = np.random.normal(scale=0.01,size=(hidden_units, hidden_units)) # hidden to hidden\n",
    "Why = np.random.normal(scale=0.01,size=(vocab_size, hidden_units)) # hidden to output\n",
    "\n",
    "\n",
    "for num in range(50):\n",
    "    epoch_acc_loss = 0\n",
    "    hprev = np.zeros((hidden_units,1)) # initlialize initila state\n",
    "\n",
    "    if optimization == 'adagrad':\n",
    "        # the 'memory is needed for adagrad , standard sgd does not use them'\n",
    "        mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "      \n",
    "    # loop in text\n",
    "    for i, (inputs, expected_outputs) in enumerate(sliding_window(data, seq_length)):\n",
    "        inputs = list(map(lambda ch:char_to_ix[ch], inputs))\n",
    "        expected_outputs = list(map(lambda ch:char_to_ix[ch], expected_outputs))\n",
    "\n",
    "        xs, hs, ps, hprev = forward_pass(inputs, hprev)\n",
    "\n",
    "        loss = cross_entropy_loss(ps, expected_outputs)\n",
    "        epoch_acc_loss+=loss\n",
    "\n",
    "        dWxh, dWhh, dWhy = backward_pass(xs, hs, ps, expected_outputs)\n",
    "\n",
    "        update_parameters() # parameters are global\n",
    "\n",
    "\n",
    "#         debug()        \n",
    "    print(num+1,'. epoch', ' average loss ', epoch_acc_loss/(i+1), sep='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natilst-of-d-pon contage, on-den conthod coll stat ms aning rom in mom gensfat contsim'ste-aling mad), contl-off-policy, colthe ficientiuct gradienting of stabs on-haing off-m's bleic wetse-polsto gene\n"
     ]
    }
   ],
   "source": [
    "print_sample(hprev, 40, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss drops but slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried running the rnn on abstracts without combining them. But nothing good to show came up in 100 epochs so i canceled them. Over all I believe that my implementation is correct. But we should not expect so much from one vanilla rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
